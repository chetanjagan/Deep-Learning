{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a754f59e-fcab-4b06-81a7-83e2be587f89",
   "metadata": {},
   "source": [
    "Refer the folder structure containing image data for two classes, **Cars** and **Bikes**, and are required to refer to this structure before beginning the implementation. The task involves loading all RGB images from both folders, resizing them to a fixed size, normalizing pixel values, and converting each image into a suitable input format for a deep neural network. Appropriate class labels must be assigned, and the combined dataset should be shuffled and split into training and testing sets in an 80:20 ratio. A deep neural network model must then be designed by specifying the input layer, one or more hidden layers with appropriate activation functions, and an output layer for binary classification. The model should be trained using forward propagation, backpropagation, and an optimization algorithm such as gradient descent or its variants, while monitoring the training loss. After training for a fixed number of epochs, predictions must be generated for both training and testing data to compute accuracy. Finally, the loss versus epochs graph should be plotted and the training and testing accuracy reported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eabbd36-9e67-433b-bc24-fa5c5d410530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a88c98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH=\"transport\"\n",
    "CATEGORIES=[\"bike\",\"cars\"]\n",
    "IMG_SIZE=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944d19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_path,categories,img_size):\n",
    "    X=[] #image data (features)\n",
    "    Y=[] #labels (0 or 1)\n",
    "    for label, category in enumerate(categories):\n",
    "        folder_path=os.path.join(base_path,category)\n",
    "        # verify if that folder exists or not\n",
    "        assert os.path.exists(folder_path) , f\"{folder_path} not found\"\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path=os.path.join(folder_path,img_name)\n",
    "            try:\n",
    "                img=Image.open(img_path).convert(\"RGB\") # Converts grayscale / RGBA → RGB\n",
    "                img=img.resize((img_size,img_size)) #Resizes image to 64×64\n",
    "                # to perform normalisation we need to convert it to numoy array first\n",
    "                img=np.array(img)/255.0\n",
    "                img=img.flatten() #mage shape before: (64, 64, 3) -> After flatten: (12288,)\n",
    "                X.append(img)\n",
    "                Y.append(label)\n",
    "            except:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "    return np.array(X) ,np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af576164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 785\n",
      "Feature vector size: 12288\n"
     ]
    }
   ],
   "source": [
    "# now lets call the function to load the dataset\n",
    "X, Y = load_dataset(DATASET_PATH, CATEGORIES, IMG_SIZE)\n",
    "print(\"Total samples:\", X.shape[0])\n",
    "print(\"Feature vector size:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc888e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (12288, 628)\n",
      "Train set shape of y: (1, 628)\n",
      "Test set shape: (12288, 157)\n",
      "Test set shape of y: (1, 157)\n"
     ]
    }
   ],
   "source": [
    "# now we have to shuffle the dataset\n",
    "indices=np.random.permutation(X.shape[0])\n",
    "X=X[indices]\n",
    "Y=Y[indices]\n",
    "# lets define the split variable\n",
    "split=int(0.8*X.shape[0])\n",
    "X_train=X[:split].T # shape of X was (200,12288) -> after transpose: (12288, 160)\n",
    "Y_train=Y[:split].reshape(1,-1) # shape of Y was (200,) -> after reshape: (1, 160)\n",
    "X_test=X[split:].T\n",
    "Y_test = Y[split:].reshape(1, -1)\n",
    "\n",
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Train set shape of y:\", Y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Test set shape of y:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29a4d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost at iteration 0:0.6947\n",
      "cost at iteration 500:0.5577\n",
      "cost at iteration 1000:0.4885\n",
      "cost at iteration 1500:0.4342\n"
     ]
    }
   ],
   "source": [
    "# parameter initialisation\n",
    "n_h=20 # number of neurons in the hidden layer\n",
    "m = X_train.shape[1]\n",
    "W1=np.random.randn(n_h,X_train.shape[0])*0.01\n",
    "b1=np.zeros((n_h,1))\n",
    "W2=np.random.randn(1,n_h)*0.01 # here the shape of Z1=(n_h,m),A1=(n_h,m) so, W2 should be (1,n_h) as W2.A1 we shud do\n",
    "b2=np.zeros((1,1))\n",
    "# why b1 has n_h,1 and b2 has 1,1 because the hidden layer has n_h neurons so there shud be n_h number of bias\n",
    "# whereas b2 is the output layer which has only one neuron so it will have only one bias term\n",
    "learning_rate=0.01\n",
    "for i in range(2000):\n",
    "    # forward pass\n",
    "    Z1=np.dot(W1,X_train)+b1\n",
    "    A1=np.maximum(0,Z1) # relu activation function\n",
    "    Z2=np.dot(W2,A1)+b2\n",
    "    A2=1/(1+np.exp(-Z2)) # sigmoid function\n",
    "\n",
    "    # lets calculate the cost\n",
    "    cost=-np.mean(Y_train*np.log(A2)+(1-Y_train)*np.log(1-A2))\n",
    "\n",
    "    # backward propagation very very important this is\n",
    "    dZ2=A2-Y_train\n",
    "    dW2=np.dot(dZ2, A1.T)/m\n",
    "    db2=np.mean(dZ2,axis=1,keepdims=True) # axis=1 coloumn wise mean\n",
    "    # without keepsims shape would be (1,) , after using that it becomes (1,1) preserves the shape\n",
    "    dA1=np.dot(W2.T,dZ2)\n",
    "    dZ1=dA1*(Z1 > 0)\n",
    "    dW1=np.dot(dZ1,X_train.T)/m\n",
    "    db1=np.mean(dZ1,axis=1,keepdims=True)\n",
    "\n",
    "    # update rule\n",
    "    W1-=learning_rate*dW1\n",
    "    b1-=learning_rate+db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    if i%500==0:\n",
    "        print(f\"cost at iteration {i}:{cost:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516ee0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 91.0828025477707\n",
      "Test Accuracy: 70.70063694267516\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def predict(X):\n",
    "    A1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "    A2 = 1 / (1 + np.exp(-(np.dot(W2, A1) + b2)))\n",
    "    return (A2 > 0.5).astype(int)\n",
    "\n",
    "train_acc= 100-np.mean(np.abs(predict((X_train)-Y_train)))*100\n",
    "test_acc  = 100 - np.mean(np.abs(predict(X_test) - Y_test)) * 100\n",
    "print(\"Train Accuracy:\", train_acc)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
